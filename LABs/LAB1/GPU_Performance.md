# GPU Performance

> <p><strong>人工智能与机器学习基础</strong>lab1</p>
> <p><strong>Author：</strong>TA：郑悟强</p>
> <p><strong>Date：</strong> 2025年9月</p>

这个文档是实验正文，有关实验环境配置和其它要求请查看仓库``README.md``文件

[TOC]

## 0. Intro

本次实验的目标是带你熟悉机器学习任务的基本流程，我们主要聚焦一类最经典的模型——线性模型，你将使用这类模型分别解决回归任务和分类任务，在此过程中熟悉机器学习相关工具的使用，为今后的学习打下基础.  

### 0.1 数据集

本次实验用到的数据集分为**可见和不可见两部分**，[数据集来源]([Rosykunai/SGEMM_GPU_performance · Datasets at Hugging Face](https://huggingface.co/datasets/Rosykunai/SGEMM_GPU_performance))，请认真阅读Dataset Card上的有关说明(**非常重要**!)

有关数据集的概括请参阅Dataset Card, 下面是对数据集各个字段的介绍：

- ``MWG``, ``NWG``：每矩阵在工作组级别的二维平铺大小：{16, 32, 64, 128}(int)
- ``KWG``：在工作组级别的二维平铺的内维度：{16, 32}(int)
- ``MDIMC``, ``NDIMC``：本地工作组大小：{8, 16, 32}(int)
- ``MDIMA``, ``NDIMB``：本地内存形状：{8, 16, 32}(int)
- ``KWI``：内核循环展开因子：{2, 8}(int)
- ``VWM``, ``VWN``：每矩阵加载和存储的向量宽度：{1, 2, 4, 8}(int)
- ``STRM``, ``STRN``：启用单线程访问片外内存的步幅：{0, 1}(int)
- ``SA``, ``SB``：每矩阵手动缓存二维工作组平铺：{0, 1}(int)
- ``Run_time``：该任务的平均运行时间

这里，助教已经帮助你做了必要的数据预处理（可能不够），具体数据存储在`data`中，分为`train.csv`和`valid.csv`

### 0.2 文件组织

下面是对各个文件的简要介绍，更详细的内容请参考注释，

- ``data_utils.py``：实现了基本的DataSet和DataLoader类
- ``basemodel.py``：定义了``LinearModel``，你需要以此为基础定义自己的模型
- ``basetrainer.py``：定义了``Trainer``，你需要以此为基础定义你的Trainer
- ``train.py``：训练核心代码，请注意查看其中涉及的超参数传递
- ``submission.py``：你需要修改并提交的文件

`submission.py`是你最终需要提交的，助教会检查其中的代码正确性。如果并非必须，请尽量尽量不要修改其他文件，如果需要修改，请写一个README文件进行说明。在评测中，助教会使用你的模型和权重，在测试数据上进行批量化脚本测试。

> 注：由于我们的封装比较规则，事实上，几乎所有的改进都可以在submission中进行，涉及到超参传递等需要改动``train.py``的部分，可以设计默认值，这样助教使用时不会出错

## 1. 使用线性回归进行GPU表现的预测

我们将Run_time作为目标，希望用线性回归的方法进行目标的预测。

### 1.1 数据预处理

首先，你需要读取我们的训练/验证数据，并提取其中的features和target。并且，你可以在这里对**features**做各种可能的预处理。但请注意：**不要对target做任何处理**，target助教已经做了必要的处理了。

+ 完成`load_and_preprocess_data` (5pt)

### 1.2 完成LinearRegressionModel

你需要补全`LinearRegressionModel`类的4个方法：

+ `__init__`定义你的模型，具体的，你需要设置你的初始的模型参数 (3pt)

+ `forward`代表前向传播，即模型进行调用，输入features输出模型的预测结果。请注意：为了让你的模型更加鲁棒，你应该考虑到实际可能调用时的输入格式有所不同，输入可能是一个batch，也可能只有一组数据。 (3pt)
+ `gradient`代表模型的梯度计算 (5pt)
+ `backpropagation`代表训练中一次完整的反向传播过程，其中包括计算梯度，更新模型权重 (5pt)

> 注：这个类是继承自`basemodel.py`中的LinearModel，推荐先仔细阅读下父类的实现

### 1.3 完成LinearRegressionTrainer

你需要补全`LinearRegressionTrainer`类的`compute_loss`函数，请注意和你的`gradient`是一致对应的 (4pt)

> 注：这个类是继承自`basetrainer.py`中的Trainer，推荐你**仔细阅读该类的实现**，尤其是其中训练的过程，考虑到这里不需要大家实现，但希望大家至少会写基本的训练过程

### 1.4 线性回归的理论解

事实上，我们知道，用MSE (Mean Squared Loss)作为优化目标的线性回归是存在解析解的。你可以在验证集上进行实现，来测试下我们的优化结果和理论解的优劣

+ 补全`linear_regression_analytic`函数 (5pt)

### 1.5 训练你的模型并测试效果

你可以使用这条命令来训练你的模型

```bash
python train.py --mode regression
```

具体的超参数设置，请见`train.py`

在训练后，你的模型权重等会被存储下来，同时也会显示在验证集上你的模型的评估指标

## 2. 使用逻辑回归进行GPU表现的分类

这里，助教已经在Dataset中提前处理好二分类任务的target处理，请放心使用

### 2.1 完成LogisticRegressionModel

你需要补全`LogisticRegressionModel`类的4个方法：

+ `__init__`定义你的模型，具体的，你需要设置你的初始的模型参数 (3pt)

+ `forward`代表前向传播，即模型进行调用，输入features输出模型的预测结果。请注意：为了让你的模型更加鲁棒，你应该考虑到实际可能调用时的输入格式有所不同，输入可能是一个batch，也可能只有一组数据。再请注意，你不需要在这一步输出分类值0/1，只需要输出你的概率分数即可/logits (3pt)
+ `gradient`代表模型的梯度计算 (5pt)
+ `backpropagation`代表训练中一次完整的反向传播过程，其中包括计算梯度，更新模型权重 (5pt)

### 2.2 完成LogisticRegressionTrainer

你需要补全`LogisticRegressionTrainer`类的`compute_loss`函数，请注意和你的`gradient`是一致对应的 (4pt)

### 2.3 训练你的模型并测试效果

你可以使用这条命令来训练你的模型

```bash
python train.py --mode classification
```

具体的超参数设置，请见`train.py`

在训练后，你的模型权重等会被存储下来，同时也会显示在验证集上你的模型的评估指标

## 3. 远远不够...

但实际上，你会发现，当前的效果可能远远达不到预期，比如说，对于线性回归任务中，MAE相比数据的均方差std0.8，以及你的$R^2$相对最优情况1的差距。你会发现，当前效果还远远不够。那么我们就要由此进行改进！

助教并不会直接提供你改进的策略，但推荐你可以从以下角度考虑：

+ 数据处理？
+ loss设计？
+ 超参数调整？
+ 优化算法？
+ 正则化处理？
+ 等等所有老师上课讲到的知识

但请注意，在你的实验报告中，必须有以下的内容：

+ 你做一定调整的动机是什么：你为什么做这个调整/改进？你发现了什么实验现象？想要实现什么效果？为什么觉得这个改进能够达到你的动机？（一定要讲清楚使用这个改进的逻辑是什么）
+ 使用这个改进后的效果如何：请给出实验结果！
+ 对这个结果的分析：是否达到了你的预期？实现了你的动机？你对此有什么分析？

最后，助教会使用你的模型在**不可见测试集**上进行测试，按照两个任务的**班级排行**来进行实验效果的打分

